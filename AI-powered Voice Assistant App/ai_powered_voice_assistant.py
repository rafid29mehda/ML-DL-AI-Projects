# -*- coding: utf-8 -*-
"""AI-powered Voice Assistant.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kh6sfuoPFZFiNQtQRFHG9hr1WlGJGmGB

# AI-Powered Voice Assistant with Whisper, Llava, and gTTS
This project demonstrates the creation of an AI voice assistant using the Whisper model by OpenAI for speech-to-text, Llava 1.5 7B for multimodal image and text understanding, and gTTS for converting text back into speech. The interface is built using Gradio to allow user interaction via audio and image inputs.

# Install Necessary Libraries
Install required libraries including transformers, BitsAndBytes for quantized models, Whisper, Gradio, and gTTS.
"""

!pip install -q -U transformers==4.37.2
!pip install -q bitsandbytes==0.41.3 accelerate==0.25.0
!pip install -q git+https://github.com/openai/whisper.git
!pip install -q gradio
!pip install -q gTTS

"""# Load and Configure Models
Load the LLaVA model for image-to-text generation using quantization to reduce the memory usage and set up the Whisper model for speech-to-text conversion.

"""

import torch
from transformers import BitsAndBytesConfig, pipeline

quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16
)

model_id = "llava-hf/llava-1.5-7b-hf"

pipe = pipeline("image-to-text",
                model=model_id,
                model_kwargs={"quantization_config": quantization_config})

import whisper
import gradio as gr
import time
import warnings
import os
from gtts import gTTS

"""# Loading and Displaying an Image
Load and display an image to be processed by the LLaVA model.

"""

from PIL import Image
image_path = "img.jpg"
image = Image.open((image_path))
image

"""# Downloading NLTK Tokenizer

"""

import nltk
nltk.download('punkt')
from nltk import sent_tokenize

"""# Locale Check and Pipeline Execution"""

import locale
print(locale.getlocale())  # Before running the pipeline
# Run the pipeline
print(locale.getlocale())  # After running the pipeline

"""# Running the Image Description Model"""

max_new_tokens = 200

prompt_instructions = """
Describe the image using as much detail as possible,
is it a painting, a photograph, what colors are predominant,
what is the image about?
"""

prompt = "USER: <image>\n" + prompt_instructions + "\nASSISTANT:"

outputs = pipe(image, prompt=prompt, generate_kwargs={"max_new_tokens": max_new_tokens})

outputs

"""# Displaying the Generated Text"""

for sent in sent_tokenize(outputs[0]["generated_text"]):
    print(sent)

warnings.filterwarnings("ignore")

"""# Whisper Model for Audio Transcription"""

import warnings
from gtts import gTTS
import numpy as np

torch.cuda.is_available()
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using torch {torch.__version__} ({DEVICE})")

import whisper
model = whisper.load_model("medium", device=DEVICE)
print(
    f"Model is {'multilingual' if model.is_multilingual else 'English-only'} "
    f"and has {sum(np.prod(p.shape) for p in model.parameters()):,} parameters."
)

"""# Utility Functions (Logging, Image Processing, and Transcription)"""

import re
import datetime
import os

## Logger file
tstamp = datetime.datetime.now()
tstamp = str(tstamp).replace(' ','_')
logfile = f'{tstamp}_log.txt'
def writehistory(text):
    with open(logfile, 'a', encoding='utf-8') as f:
        f.write(text)
        f.write('\n')
    f.close()

import re
import requests
from PIL import Image

def img2txt(input_text, input_image):

    # load the image
    image = Image.open(input_image)

    writehistory(f"Input text: {input_text} - Type: {type(input_text)} - Dir: {dir(input_text)}")
    if type(input_text) == tuple:
        prompt_instructions = """
        Describe the image using as much detail as possible. You are a helpful AI assistant who is able to answer questions about the image. What is the image all about? Now generate the helpful answer.
        """
    else:
        prompt_instructions = """
        Act as an expert in imagery descriptive analysis, using as much detail as possible from the image, respond to the following prompt:
        """ + input_text

    writehistory(f"prompt_instructions: {prompt_instructions}")
    prompt = "USER: <image>\n" + prompt_instructions + "\nASSISTANT:"

    outputs = pipe(image, prompt=prompt, generate_kwargs={"max_new_tokens": 200})

    if outputs is not None and len(output[0]["generated_text"]) > 0:
        match = re.search(r'ASSISTANT: (.*)', outputs[0]["generated_text"])
        if match:
          reply = match.group(1)
        else:
          reply = "No response found."
    else:
        reply = "No response generated."

    return reply

def transcribe(audio):

    # Check if the audio input is None or empty
    if audio is None or audio == '':
        return ('','',None)  # Return empty strings and None audio file

    # language = 'en'

    audio = whisper.load_audio(audio)
    audio = whisper.pad_or_trim(audio)

    mel = whisper.log_mel_spectrogram(audio).to(model.device)

    _, probs = model.detect_language(mel)

    options = whisper.DecodingOptions()
    result = whisper.decode(model, mel, options)
    result_text = result.text

    return result_text

def text_to_speech(text, file_path):
    language = 'en'

    audioobj = gTTS(text = text,
                    lang = language,
                    slow = False)

    audioobj.save(file_path)

    return file_path

import locale
locale.getpreferredencoding = lambda: "UTF-8"

!ffmpeg -f lavfi -i anullsrc=r=44100:cl=mono -t 10 -q:a 9 -acodec libmp3lame Temp.mp3

"""# Gradio Interface"""

import gradio as gr
import os
from gtts import gTTS
from PIL import Image
import whisper
import re

# Load Whisper model
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
whisper_model = whisper.load_model("medium", device=DEVICE)

# Transcribe function using Whisper
def transcribe(audio_path):
    if audio_path is not None:
        audio = whisper.load_audio(audio_path)
        audio = whisper.pad_or_trim(audio)
        mel = whisper.log_mel_spectrogram(audio).to(whisper_model.device)

        options = whisper.DecodingOptions()
        result = whisper.decode(whisper_model, mel, options)
        return result.text
    return "No audio provided."

# Corrected img2txt function using LLaVA model
def img2txt(speech_text, image_path):
    if image_path is not None:
        image = Image.open(image_path)

        # Generate a descriptive prompt for the image based on the speech input
        prompt_instructions = f"""
        Describe the image using as much detail as possible based on the following input: {speech_text}.
        """
        prompt = "USER: <image>\n" + prompt_instructions + "\nASSISTANT:"

        # Run the pipeline with the image and the generated prompt
        outputs = pipe(image, prompt=prompt, generate_kwargs={"max_new_tokens": 200})

        if outputs and len(outputs[0]["generated_text"]) > 0:
            # Extract the assistant's response part
            assistant_response = re.search(r'ASSISTANT: (.*)', outputs[0]["generated_text"])
            if assistant_response:
                return assistant_response.group(1)
        return "No description generated."
    return "No image provided."

# Function for converting text to speech using gTTS
def text_to_speech(text, output_audio_path):
    if text:
        tts = gTTS(text)
        tts.save(output_audio_path)
        return output_audio_path
    return None

# A function to handle audio and image inputs
def process_inputs(audio_path, image_path):
    # Transcribe the audio file
    speech_to_text_output = transcribe(audio_path)

    # Handle the image input and generate a response
    if image_path:
        chatgpt_output = img2txt(speech_to_text_output, image_path)
    else:
        chatgpt_output = "No image provided."

    # Convert the generated assistant's response to speech
    processed_audio_path = text_to_speech(chatgpt_output, "Temp3.mp3")

    return speech_to_text_output, chatgpt_output, processed_audio_path

# Create the Gradio interface
iface = gr.Interface(
    fn=process_inputs,
    inputs=[
        gr.Audio(type="filepath"),  # Upload audio file
        gr.Image(type="filepath")   # Upload image file
    ],
    outputs=[
        gr.Textbox(label="Speech to Text"),
        gr.Textbox(label="Generated Output from Image"),
        gr.Audio(label="Generated Audio Response")
    ],
    title="Audio & Image Processor with Whisper and LLaVA",
    description="Upload an image and interact via voice input and audio response."
)

# Launch the Gradio interface
iface.launch(debug=True)